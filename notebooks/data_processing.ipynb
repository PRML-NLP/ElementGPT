{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import jsonlines\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "data_list = [\n",
    "    \"KoAlpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"KoChatGPT/kochatgpt_1_SFT.jsonl\",\n",
    "    \"korquad-chat-v1/korquad-chat.json\",\n",
    "    \"OIG-small-chip2-ko/oig-smallchip2-dedu.jsonl\",\n",
    "    \"ShareGPT_DeepL/ko_alpaca_style_dataset.json\",\n",
    "    \"kullm_v2/kullm-v2.jsonl\",\n",
    "    \"KoAlpaca/ko_alpaca_data.json\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoAlpaca v1.0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49620"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[-1]\n",
    "\n",
    "with open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    koalpaca_v1_0 = json.load(f)\n",
    "\n",
    "len(koalpaca_v1_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoAlpaca v1.1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[0]\n",
    "\n",
    "koalpaca_v1_1 = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        koalpaca_v1_1.append(line)\n",
    "\n",
    "len(koalpaca_v1_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KochatGPT SFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[1]\n",
    "with open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    kochatgpt = json.load(f)\n",
    "\n",
    "len(kochatgpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korquad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9619"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[2]\n",
    "\n",
    "korquad = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        korquad.append(line)\n",
    "\n",
    "len(korquad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OIG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210282"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[3]\n",
    "\n",
    "oig = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        oig.append(line)\n",
    "\n",
    "len(oig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KULLM-v2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152630"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[5]\n",
    "\n",
    "kullm = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        kullm.append(line)\n",
    "\n",
    "len(kullm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI HUB BOOK SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "json_files = glob(\"../data/AIhub_book_summary/*.json\")\n",
    "\n",
    "ins_format = \"다음 문단은 \\'{kdc_label}\\'로 분류된 문서의 일부분이다.\\n\\n{passage}\\n\\n이 문단을 간단히 요약하고 해당 문서의 제목을 지어줘\"\n",
    "res_format = \"문단 요약: {summary}\\n\\n{kdc_label} {doc_type} 제목 제안: \\\"{doc_name}\\\"\"\n",
    "\n",
    "book_summary = []\n",
    "for path in tqdm(json_files):\n",
    "    with open(path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    data = json_data[\"metadata\"]\n",
    "    if \"교육\" in data[\"kdc_label\"]:\n",
    "        data[\"passage\"] = json_data[\"passage\"]\n",
    "        data[\"summary\"] = json_data[\"summary\"]\n",
    "        ins = ins_format.format_map(data)\n",
    "        res = res_format.format_map(data)\n",
    "        book_summary.append({\"instruction\":ins, \"output\":res})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAVERPEDIA_수업자료 생성 scenario\n",
    "\n",
    "우리아이 성장백과 -> 영양 과목 \\\n",
    "어린이백과_사회, 어린이백과_한국사, 어린이백과_세계탐구, 어린이백과_유적/유물 탐구 -> 사회 과목 \\\n",
    "어린이백과_과학, 어린이백과_과학탐구 -> 과학 과목 \\\n",
    "어린이백과_세계사 -> 세계사 과목 \\\n",
    "어린이백과_수학 -> 수학 과목 \\\n",
    "어린이백과_소프트웨어 -> 컴퓨터 과목\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.91it/s]\n"
     ]
    }
   ],
   "source": [
    "#영양, 사회, 세계사, 수학, 과학, 컴퓨터\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "naverpedia_json_files = glob(\"/workspace/ElementGPT/data/web_crawled/naverpedia_class.json\")\n",
    "#format for 성장백과, 사회, 한국사, 세계사, 수학, 과학 \n",
    "ins_format1 = \"USER: 초등학생을 대상으로 \\'{title}\\'이라는 주제로 수업을 진행하려고해.\\{category}\\이라는 점을 고려해서 수업 자료를 작성해줘\"\n",
    "res_format1 = \"Assistant: {contents}\"\n",
    "\n",
    "for path in tqdm(naverpedia_json_files):\n",
    "    with open(path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "naverpedia= []\n",
    "for i in range(len(json_data)):\n",
    "    ins = ins_format1.format_map(json_data[i])\n",
    "    res = res_format1.format_map(json_data[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"USER: 초등학생을 대상으로 '인터넷 중독 [Internet Addiction]'이라는 주제로 수업을 진행하려고해.\\\\컴퓨터 과목\\\\이라는 점을 고려해서 수업 자료를 작성해줘\", 'output': 'Assistant: ## 예시\\n인터넷 중독 증상에 대하여 다음과 같은 인터넷 사이트를 통하여 진단할 수 있습니다.https://www.iapc.or.kr/dia/survey/addDiaSurveyNew.do?dia_type_cd=IABO만약 중독 증상을 보인다면 전문가와 상담하여 치료를 받는 것이 좋습니다. 중독 증상에 대하여 교육이나 상담을 할 수 있는 곳은 다음과 같습니다.스마트 쉼 센터 1599-0075## 설명\\n인터넷에 중독되면 인터넷을 사용하지 않을 때 불안해지며 습관적으로 인터넷을 사용하게 됩니다. 또한 건강을 해치고 공부에 신경을 덜 쓰게 되며 친구 및 가족 관계에 소홀해지게 됩니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(naverpedia[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAVERPEDIA_manual하게 선별 필요 부분\n",
    "##어린이백과_국어, 영어 \\\n",
    "어린이백과_예체능 \\\n",
    "##어린이백과_시사/논술 \\\n",
    "##어린이백과_문학 탐구 \\\n",
    "##어린이백과_인물 탐구 \\\n",
    "##어린이백과_지식e \\\n",
    "##어린이백과_천재학습백과\\\n",
    " tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.70it/s]\n"
     ]
    }
   ],
   "source": [
    "naverpedia_json_files = glob(\"/workspace/ElementGPT/data/web_crawled/naverpedia.json\")\n",
    "\n",
    "for path in tqdm(naverpedia_json_files):\n",
    "    with open(path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "\n",
    "def get_dictionary_by_category(json, category):\n",
    "    list = []\n",
    "    for item in json:\n",
    "        if \"category\" in item and item[\"category\"] ==category:\n",
    "            list.append(item)\n",
    "    return list\n",
    "\n",
    "#format for 어린이백과 시사/논술 \n",
    "ins_format2 = \"USER: \\'{title}\\'이라는 주제에 관련해서 실생활에서 볼 수 있는 사례를 들면서 설명하려고해.'\\{title}\\'에 관한 사례를 들고 구체적인 설명을 만들어줘\"\n",
    "res_format2 = \"Assistant: {contents}\"\n",
    "kidspedia_essay = get_dictionary_by_category(json_data, \"어린이백과_시사/논술\")\n",
    "\n",
    "for i in range(len(kidspedia_essay)):\n",
    "    ins = ins_format2.format_map(kidspedia_essay[i])\n",
    "    res = res_format2.format_map(kidspedia_essay[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "#format for 어린이백과 국어\n",
    "ins_format3 = \"USER: 국어사전을 작성하려고 하는데 \\'{title}\\'이라는 말의 뜻이나 실생활에서 사용하는 사례를 설명해줘\"\n",
    "res_format3 = \"Assistant: {contents}\"\n",
    "kidspedia_korean = get_dictionary_by_category(json_data, \"어린이백과_국어\")\n",
    "\n",
    "for i in range(len(kidspedia_korean)):\n",
    "    ins = ins_format3.format_map(kidspedia_korean[i])\n",
    "    res = res_format3.format_map(kidspedia_korean[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "#format for 어린이백과 영어\n",
    "ins_format4 = \"USER: \\'{title}\\'이라는 주제에 관련해서 영어 수업자료를 만들고 있어. 실제로 사용하는 사례를 들어서 만들어줘\"\n",
    "res_format4 = \"Assistant: {contents}\"\n",
    "kidspedia_english = get_dictionary_by_category(json_data, \"어린이백과_영어\")\n",
    "\n",
    "for i in range(len(kidspedia_english)):\n",
    "    ins = ins_format4.format_map(kidspedia_english[i])\n",
    "    res = res_format4.format_map(kidspedia_english[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "#format for 어린이백과 문학탐구\n",
    "ins_format5 = \"USER: \\'{title}\\'이라는 제목을 가진 어린이한테 교훈을 줄 수 있는 우화를 만들어줘\"\n",
    "res_format5 = \"Assistant: {contents}\"\n",
    "kidspedia_story = get_dictionary_by_category(json_data, \"어린이백과_문학탐구\")\n",
    "\n",
    "for i in range(len(kidspedia_story)):\n",
    "    ins = ins_format5.format_map(kidspedia_story[i])\n",
    "    res = res_format5.format_map(kidspedia_story[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "#format for 어린이백과 인물탐구\n",
    "ins_format6 = \"USER: \\'{title}\\'이라는 인물의 생애와 업적을 소개하는 글을 작성해줘\"\n",
    "res_format6 = \"Assistant: {contents}\"\n",
    "kidspedia_celebrity = get_dictionary_by_category(json_data, \"어린이백과_인물탐구\")\n",
    "\n",
    "for i in range(len(kidspedia_celebrity)):\n",
    "    ins = ins_format6.format_map(kidspedia_celebrity[i])\n",
    "    res = res_format6.format_map(kidspedia_celebrity[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "#format for 어린이백과 지식e\n",
    "ins_format7 = \"USER: 초등학생을 대상으로 \\'{title}\\'이라는 주제로 수업을 진행하려고해.\\{title}\\에 관련해서 어린이를 위한 내용으로 자료를 작성해줘\"\n",
    "res_format7 = \"Assistant: {contents}\"\n",
    "kidspedia_knowledge = get_dictionary_by_category(json_data, \"어린이백과_지식e\")\n",
    "\n",
    "for i in range(len(kidspedia_knowledge)):\n",
    "    ins = ins_format7.format_map(kidspedia_knowledge[i])\n",
    "    res = res_format7.format_map(kidspedia_knowledge[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "#format for 어린이백과 천재학습백과\n",
    "ins_format8 = \"USER: 초등학생을 대상으로 \\'{title}\\'이라는 주제로 수업을 진행하려고해.\\{title}\\에 관련해서 어린이를 위한 내용으로 자료를 작성해줘\"\n",
    "res_format8 = \"Assistant: {contents}\"\n",
    "kidspedia_genius = get_dictionary_by_category(json_data, \"어린이백과_천재학습백과\")\n",
    "\n",
    "for i in range(len(kidspedia_genius)):\n",
    "    ins = ins_format8.format_map(kidspedia_genius[i])\n",
    "    res = res_format8.format_map(kidspedia_genius[i])\n",
    "    naverpedia.append({\"instruction\":ins, \"output\":res})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"USER: '[스물네 고개] 과학'이라는 주제에 관련해서 실생활에서 볼 수 있는 사례를 들면서 설명하려고해.'\\\\[스물네 고개] 과학'에 관한 사례를 들고 구체적인 설명을 만들어줘\", 'output': 'Assistant: 1. 인공위성이 돌기 위해서는 계속하여 에너지를 제공해야 할까?① 계속 에너지 공급이 필요해.② 한 번 돌기 시작하면 새로운 에너지가 필요 없어.2. 10억 분의 1미터로 원자 수준의 크기에 해당하는 길이의 단위는?① 나노미터② 밀리미터③ 마이크로미터3. 달에 처음 인류를 착륙시킨 우주선은 무엇일까?4. 인류가 지구 이외의 행성에 가까이 간 적이 있을까?① 달 이외의 천체 가까이 간 적이 없어.② 화성에 간 적이 있어.5. 다음은 일식과 월식에 대한 설명이야. 어울리는 것끼리 연결해 봐.6. 다음은 현미경에 관한 설명이야.원자 간의 미는 힘을 이용하여 캔틸레버라고 하는 작은 막대의 휘어지는 정도를 통해 물체를 측정하는 현미경을 ( )이라고 해.7. 달이 뜨는 시각이 매일 변하고 모양이 변하는 이유는 무엇일까?① 달이 지구의 둘레를 공전하기 때문② 달이 자전하기 때문③ 달의 모양이 변하기 때문8. 투발루가 바닷속으로 가라앉고 있는 이유에 해당하는 것은?① 공기 중에 이산화탄소가 많아지기 때문② 태양이 점점 뜨거워지고 있기 때문9. 다음은 인간형 로봇에 대한 설명이야. 알맞은 것끼리 짝지어 봐.10. 다음은 정보통신기술에 대한 설명이야.디지털 기술 기반의 여러 제품이나 서비스가 결합해서 새로운 형태로 탄생하는 것을 ( )이라고 해.11. DNA, 염색체, 염색사에 대한 설명이야. 알맞은 것끼리 연결해 봐.12. 언제, 어디서나 편리하게 컴퓨팅 자원을 활용할 수 있도록 현실세계와 가상세계를 결합시킨 것을 무엇이라고 할까?13. 최초의 생명체는 자연적으로 만들어질 수 있는가?밀러와 유레이는 실험실에서 생명체를 이루는 단백질의 성분인 ( )을 만드는 데 성공하였어.14. 다윈은 자연 환경의 변화에 효과적으로 변화한 생물체가 살아남을 수 있다는 ○○○을 발표했어.15. 그동안 발견된 화석 중에서 파충류에서 조류로 진화했음을 보여주는 것은?① 암모나이트 화석② 삼엽충 화석③ 시조새 화석16. 바이러스는 생물일까? 바이러스는 번식은 하지만 먹이를 먹지 않고 자랄 수 없대.① 무생물로 분류해.② 생물로 분류해.17. 광우병을 스펀지형 뇌질환이라고 하는 이유는 무엇일까?광우병의 원인 물질인 ( )은 뇌세포를 파괴하여 스펀지처럼 구멍이 숭숭 뚫린 상태로 만들어.18. IT, BT, NT에 대한 설명이야. 알맞은 것끼리 짝지어 봐.19. 3대 영양소에 대한 설명이야. 알맞은 것끼리 연결해 봐.20. 화석연료의 문제점에는 어떤 것이 있을까?화석연료가 연소하면 반드시 나오는 ( )는 지구 온난화의 주범이고, 연료에 들어 있는 황이 연소할 때 나오는 ( )는 공기오염을 일으켜.21. 태양광 발전일까, 태양열 장치일까?22. 산성비와 호흡기 질환을 일으키는 물질은?① 아황산가스② 이산화탄소23. 깊은 바다에 숨어 있는 귀중한 자원은 무엇일까?깊은 바다에는 귀중한 금속을 함유한 ( )와 연료물질인 메탄가스 고체인 ( )가 풍부해.24. 화학비료나 농약을 사용하지 않는 농사법을 무엇이라고 할까?▶ 정답 확인 ( ↓ 아랫부분을 클릭해주세요.)1. ② 2. ① 3. 아폴로 4. ① 5. 달이 태양을 가리는 현상 → 일식, 지구의 그림자 속으로 달이 들어가는 현상 → 월식 6. 원자힘현미경 7. ① 8. ① 9. 모습과 행동이 인간을 닮은 로봇 → 안드로이드, 겉모습이 사람과 닮아 머리, 몸통, 팔, 다리가 있는 로봇 → 휴머노이드 10. 디지털 융합 11. 세포 분열 때 염색되어 나타나는 막대 모양으로 사람은 23쌍 갖고 있어. → 염색체, 염색체가 풀어져 실처럼 보이는 것 → 염색사, 한 생명체가 갖고 있는 모든 염색체의 유전정보 → 게놈 12. 유비쿼터스 13. 아미노산 14. 진화론 15. ③ 16. ① 17. 프리온 18. 생명체가 갖고 있는 기능을 이용하는 기술 → BT, 정보통신 산업의 발전과 함께 정보혁명을 주도하는 기술 → IT, 원자 단위의 아주 미세한 수준의 정밀도를 요구하는 기술 → NT 19. 물과 탄소로 이루어진 영양소 → 탄수화물, 탄소와 수소로 이루어진 높은 에너지를 내는 영양소 → 지방, 아미노산으로 이루어지고 세포에서 중요한 기능을 담당하는 영양소 → 단백질 20. 이산화탄소, 아황산가스 21. 태양에너지를 이용하여 전기를 만들어 이용 → 태양광 발전, 태양열을 이용하여 물을 데워 이용 → 태양열 장치 22. ① 23. 망간단괴, 하이드레이트 24. 유기농업'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?', 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}\n",
      "{'prompt': '불고기용 고기 한우에요?', 'completion': '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.', 'tokens': 193}\n",
      "{'source': 'korquad-chat', 'text': '<sys>1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\\n<usr> 바그너가 1839년에 파우스트를 소재로 한 교향곡 작곡을 시작했다는데, 왜 이 소재에 마음이 끌렸을까요?\\n<bot> 바그너는 파우스트의 메피스토펠레스를 만나는 파우스트의 심경에 공감했기 때문입니다. 또한 바그너는 파리에서 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았고, 이것이 이 작품에 조금 영향을 끼쳤다고 합니다.\\n<usr> 작곡을 시작한 이후 작업은 어떻게 진행됐나요?\\n<bot> 작곡을 시작한 1839년부터 40년에 걸쳐 파리에서 작업을 시작했지만, 1악장을 쓴 뒤 중단하게 됐습니다. 그리고 이 간 동안 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 바쁜 시간을 보냈는데 이런 바쁜 생활이 이 곡을 잊게 한 것이 아닐까 추측되고 있습니다.\\n<usr> 그렇다면 이 작품이 연주되지 않은 이유는 무엇인가요?\\n<bot> 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였지만, 실제로는 이루어지지 않았습니다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고, 재연도 이루어졌지만, 이후에는 그대로 방치되고 말았습니다.'}\n",
      "{'user': \"I've heard that it's a good idea to have a will. What is a will?\\n\\n\", 'chip2': \"A will is a legal document that specifies how your property should be distributed after you die. It can also specify who should care for any children or other dependents you may have. It's important to make sure that your will is valid and up-to-date, since the laws governing wills vary from state to state.\", 'index': 0, 'user_translated': '유언장이 있는 것이 좋다는 말을 들었습니다. 유언장이란 무엇입니까?\\n\\n', 'chip2_translated': '유언장은 귀하가 사망한 후 귀하의 재산이 어떻게 분배되어야 하는지를 지정하는 법적 문서입니다. 또한 귀하가 가질 수 있는 자녀나 기타 부양가족을 누가 돌봐야 하는지 명시할 수 있습니다. 유언장에 적용되는 법률이 주마다 다르기 때문에 귀하의 유언장이 유효하고 최신인지 확인하는 것이 중요합니다.'}\n",
      "{'id': 'vicuna_{idx}', 'instruction': '이것이 난수를 생성하는 가장 좋은 방법일까요?const code = Math.floor(100000 + Math.random() \\\\* 900000);', 'input': '', 'output': \"제공한 코드입니다:```const code = Math.floor(100000 + Math.random() * 900000);```는 100000에서 999999 사이의 난수를 생성하므로 사용 사례에 적합할 수 있습니다. 그러나 이 방법은 난수를 생성하는 암호학적으로 안전한 방법으로 간주되지 않습니다.대신 암호학적으로 안전한 난수 생성기(CSPRNG)를 사용하는 것이 좋습니다. Node.js의 `crypto` 모듈은 암호학적으로 강력한 의사 난수 데이터를 생성하는 `randomBytes()` 함수를 제공합니다.다음은 `crypto.randomBytes()`를 사용하여 임의의 6자리 코드를 생성하는 방법의 예입니다:```const crypto = require('crypto');const code = crypto.randomBytes(3).toString('hex');console.log(code);```이렇게 하면 6개의 16진수 문자(0-9 및 a-f)로 구성된 임의의 문자열이 생성되며, 이 문자열을 코드로 사용할 수 있습니다.생성된 난수는 비밀로 유지되어야 하며 일반 텍스트로 저장해서는 안 된다는 점을 명심해야 합니다.\"}\n"
     ]
    }
   ],
   "source": [
    "# print(koalpaca_v1_0[0])\n",
    "print(koalpaca_v1_1[0])\n",
    "print(kochatgpt[0])\n",
    "print(korquad[0])\n",
    "print(oig[0])\n",
    "# print(sharegpt[0])\n",
    "print(kullm[-11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unifying except for Korquad dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21155/21155 [00:00<00:00, 163113.84it/s]\n",
      "100%|██████████| 12000/12000 [00:00<00:00, 171716.49it/s]\n",
      "100%|██████████| 210282/210282 [00:01<00:00, 172192.66it/s]\n",
      "100%|██████████| 152630/152630 [00:00<00:00, 199452.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "381020"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def to_dialog(data, inst_key, out_key, preprossing=True):\n",
    "    dialog = []\n",
    "    for sample in tqdm(data, total=len(data)):\n",
    "        if preprossing:\n",
    "            inst = re.sub(r\"\\?(\\w)\", r\"?\\n\\1\", sample[inst_key])\n",
    "            output = re.sub(r\"[.](\\d+)[.]\", r\".\\r\\n\\1.\", sample[out_key])\n",
    "        \n",
    "        dialog.append(\n",
    "            [{\"from\": \"human\", \"value\": inst},\n",
    "             {\"from\": \"bot\", \"value\": output}]\n",
    "        )\n",
    "    \n",
    "    return dialog\n",
    "        \n",
    "\n",
    "print(\"Unifying except for Korquad dataset\")\n",
    "unified_data = []\n",
    "\n",
    "# koalpacav1.1\n",
    "unified_data.extend(to_dialog(koalpaca_v1_1, \"instruction\", \"output\", \"general\"))\n",
    "# kochatgpt\n",
    "unified_data.extend(to_dialog(kochatgpt, \"prompt\", \"completion\", \"general\"))\n",
    "# OIG\n",
    "unified_data.extend(to_dialog(oig, \"user_translated\", \"chip2_translated\", \"general\"))\n",
    "    \n",
    "# KULLM-v2\n",
    "processed_kullm = []\n",
    "for sample in tqdm(kullm, total=len(kullm)):\n",
    "    if sample[\"input\"]:\n",
    "        inst = sample[\"instruction\"].strip()+\"\\n\\n\"+sample[\"input\"].strip()\n",
    "    else:\n",
    "        inst = sample[\"instruction\"].strip()\n",
    "        \n",
    "    if len(inst) <= 15:\n",
    "        continue\n",
    "    \n",
    "    output = re.sub(r\"[.](\\d+)[.]\", r\".\\r\\n\\1.\", sample[\"output\"])\n",
    "        \n",
    "    processed_kullm.append(\n",
    "        [{\"from\": \"human\", \"value\": inst},\n",
    "         {\"from\": \"bot\", \"value\": output}]\n",
    "    )\n",
    "print(len(processed_kullm))\n",
    "unified_data.extend(processed_kullm)\n",
    "\n",
    "# unified_data.extend(koalpaca_v1_0)\n",
    "len(unified_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Korquard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390248"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in korquad:#, total=len(korquad)):\n",
    "    try:\n",
    "        turns = sample[\"text\"].split(\"\\n\")\n",
    "        context = turns[0].split(\">\")[1].strip()\n",
    "        dialog = []\n",
    "        for turn in turns[1:]:\n",
    "            splited = turn.split(\">\")\n",
    "            role = splited[0][1:]\n",
    "            text = splited[1].strip()\n",
    "            dialog.append({\"role\": role, \"text\": text})\n",
    "            \n",
    "        assert dialog[0][\"role\"]==\"usr\" and len(dialog)%2==0\n",
    "        \n",
    "        inst_res = []\n",
    "        for i in range(0, len(dialog), 2):\n",
    "            user = dialog[i][\"text\"]\n",
    "            if i==0:\n",
    "                user = context.strip()+\"\\n\\n\"+user.strip()\n",
    "            bot = dialog[i+1][\"text\"]\n",
    "        \n",
    "            inst_res.extend(\n",
    "                [{\"from\": \"human\", \"value\": user},\n",
    "                 {\"from\": \"bot\", \"value\": bot}]\n",
    "            )\n",
    "        \n",
    "        unified_data.append(inst_res)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "len(unified_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Hub Book summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180001/180001 [00:05<00:00, 30245.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "json_files = glob(\"../data/AIhub_book_summary/*.json\")\n",
    "\n",
    "ins1_format = \"다음 문단은 \\'{kdc_label}\\'로 분류된 문서의 일부분이다.\\n\\n{passage1}\\n\\n이 문단에 이어질 한 문장을 작성해줘\"\n",
    "res1_format = \"\\\"{passage2}\\\"가 될 수 있겠습니다.\"\n",
    "ins2_format = \"완성된 문단을 간단히 요약하고 해당 문서의 제목을 지어줘\"\n",
    "res2_format = \"이 문단은 다음과 같이 요약될 수 있습니다.\\r\\n- 요약: {summary}\\r\\n\\r\\n- {kdc_label} {doc_type} 제목 제안: \\\"{doc_name}\\\"\"\n",
    "\n",
    "target_kdc = [\"보건의료\", \"체육\", \"보육·가족및여성\", \"한국문학\", \"한국어\", \"문학\", \"문화재\", \"역사\", \"음악\"]\n",
    "book_summary = []\n",
    "kdcs = []\n",
    "for path in tqdm(json_files):\n",
    "    with open(path, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "    data = json_data[\"metadata\"]\n",
    "    kdcs.append(data[\"kdc_label\"])\n",
    "        \n",
    "    if \"교육\" in data[\"kdc_label\"] or data[\"kdc_label\"] in target_kdc:\n",
    "        splited_passage = json_data[\"passage\"].split(\". \")\n",
    "        data[\"passage1\"] = \". \".join(splited_passage[:-1])+\".\"\n",
    "        data[\"passage2\"] = splited_passage[-1]\n",
    "        data[\"summary\"] = json_data[\"summary\"]\n",
    "        ins1 = ins1_format.format_map(data)\n",
    "        res1 = res1_format.format_map(data)\n",
    "        ins2 = ins2_format.format_map(data)\n",
    "        res2 = res2_format.format_map(data)\n",
    "        book_summary.append([\n",
    "            {\"from\":\"human\", \"value\":ins1},\n",
    "            {\"from\":\"bot\", \"value\":res1},\n",
    "            {\"from\":\"human\", \"value\":ins2},\n",
    "            {\"from\":\"bot\", \"value\":res2},\n",
    "        ])\n",
    "    #     data[\"passage\"] = json_data[\"passage\"]\n",
    "    #     data[\"summary\"] = json_data[\"summary\"]\n",
    "    #     ins = ins_format.format_map(data)\n",
    "    #     res = res_format.format_map(data)\n",
    "    #     kdcs.append(data[\"kdc_label\"])\n",
    "    #     book_summary.append({\"instruction\":ins, \"output\":res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422474"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unified_data.extend(book_summary)\n",
    "len(unified_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI hub 주제별 일상 대화 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/AIhub_chitchat/KAKAO_1648_13.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98651"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "chitchat_list = glob(\"../data/AIhub_chitchat/*.json\")\n",
    "chitchat_data = []\n",
    "\n",
    "for path in chitchat_list:\n",
    "    try:\n",
    "        with open(path, \"r\") as fin:\n",
    "            chitchat_data.append(json.load(fin))\n",
    "    except Exception:\n",
    "        print(path)\n",
    "        continue\n",
    "\n",
    "len(chitchat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70613\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "dialogs = []\n",
    "for data in chitchat_data:\n",
    "    data = data[\"info\"][0]\n",
    "    domain = data[\"annotations\"][\"speaker_type\"]\n",
    "    if data[\"annotations\"][\"speaker_type\"]!=\"1:1\":\n",
    "        continue\n",
    "    \n",
    "    data = data[\"annotations\"]\n",
    "    lines = data[\"lines\"]\n",
    "    \n",
    "    if len(lines) < 2 or not data[\"lines\"][0][\"norm_text\"]:\n",
    "        continue\n",
    "    \n",
    "    if len(set([line[\"speaker\"][\"id\"] for line in lines]))!=2:\n",
    "        continue\n",
    "       \n",
    "    user_info = lines[0][\"speaker\"]\n",
    "    u_id = user_info[\"id\"]\n",
    "    u_gender = user_info[\"sex\"]\n",
    "    u_age = user_info[\"age\"]\n",
    "    \n",
    "    prev_id = u_id\n",
    "    dialog = []\n",
    "    user = []\n",
    "    bot = []\n",
    "    for i in range(len(lines)):\n",
    "        cur_id = lines[i][\"speaker\"][\"id\"]\n",
    "        if prev_id!=cur_id and prev_id!=u_id:\n",
    "            response = \" \".join(bot).strip()\n",
    "            response = re.sub(\"키키\", \"ㅋㅋ\", response)\n",
    "            dialog.extend(\n",
    "                [{\"from\": \"human\", \"value\": \" \".join(user).strip()},\n",
    "                 {\"persona\": {\"age\": a_age, \"gender\": a_gender, \"domain\": \"일상 대화\"}, \"from\": \"bot\", \"value\": response}]\n",
    "            )\n",
    "            user = []\n",
    "            bot = []\n",
    "\n",
    "        if cur_id!=u_id:\n",
    "            a_gender = lines[i][\"speaker\"][\"sex\"]\n",
    "            a_age = lines[i][\"speaker\"][\"age\"]\n",
    "            bot.append(lines[i][\"norm_text\"])\n",
    "        else:\n",
    "            user.append(lines[i][\"norm_text\"])\n",
    "            \n",
    "        prev_id = cur_id\n",
    "    \n",
    "    if dialog:\n",
    "        dialogs.append(dialog.copy())\n",
    "    \n",
    "print(len(dialogs))\n",
    "\n",
    "unified_data.extend(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493087"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    AutoConfig,\n",
    "    CONFIG_MAPPING\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from model_utils.conversation import get_conv_template, SeparatorStyle\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv_g = get_conv_template(\"elementgpt_for_general\")\n",
    "    conv_p = get_conv_template(\"elementgpt_for_persona\")\n",
    "    roles = {\"human\": conv_g.roles[0], \"bot\": conv_g.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        print(source)\n",
    "        if \"persona\" in source[1]:\n",
    "            conv = conv_p.copy()\n",
    "            conv.system = conv.system.format_map(source[1][\"persona\"])\n",
    "        else:\n",
    "            conv = conv_g\n",
    "            \n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "        \n",
    "    return conversations\n",
    "    \n",
    "    # Tokenize conversations\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "    for i, conversation in enumerate(tqdm(conversations, desc=\"Tokenizing inputs\")):\n",
    "        input_ids_sample = tokenizer(\n",
    "            conversation,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        input_ids.append(input_ids_sample)\n",
    "        targets.append(input_ids_sample.clone())\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    print(\"Masking targets...\")\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in tqdm(zip(conversations, targets), total=len(conversations)):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum()) + 1\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            round_len = len(tokenizer(rou).input_ids)\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        if False:\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_INDEX\n",
    "                print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "                print(len(rounds), conversation)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'human', 'value': '일본 대마도에서 절도범들이 훔쳐 들어온 금동불상 얘기 들었어?'}, {'persona': {'age': '30대', 'gender': '여성', 'domain': '일상 대화'}, 'from': 'bot', 'value': '일본 어디서 가져왔는데?'}, {'from': 'human', 'value': '가져온 게 아니고, 훔쳤는데... 관음사에서.'}, {'persona': {'age': '30대', 'gender': '여성', 'domain': '일상 대화'}, 'from': 'bot', 'value': '훔쳤다면 돌려줘야겠네.'}, {'from': 'human', 'value': '근데 그게 진품이라서... 부석사에서 불상에 대한 소유권을 주장하고...'}, {'persona': {'age': '30대', 'gender': '여성', 'domain': '일상 대화'}, 'from': 'bot', 'value': '우리나라에서는 그래도 일본에 돌려줘야할텐데'}, {'from': 'human', 'value': '그래서 부석사가 정부 상대로 민사소소을 제기했다네...'}, {'persona': {'age': '30대', 'gender': '여성', 'domain': '일상 대화'}, 'from': 'bot', 'value': '음... 일본이 훔쳐갔던거라면 다시 돌려받아야지'}, {'from': 'human', 'value': '정식으로 돌려받았다면 좋았을텐데... 이게 다시 훔친 거라서... ;'}, {'persona': {'age': '30대', 'gender': '여성', 'domain': '일상 대화'}, 'from': 'bot', 'value': '아무튼 잘 해결되었음 좋겠다.'}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"EleutherAI/polyglot-ko-5.8b\",\n",
    "    cache_dir=None,\n",
    "    model_max_length=1024,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "processed = preprocess(dialogs[15:16], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/unified_instruction_add_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unified_data, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493087"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
