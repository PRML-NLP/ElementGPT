{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import jsonlines\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "data_list = [\n",
    "    \"KoAlpaca/KoAlpaca_v1.1.jsonl\",\n",
    "    \"KoChatGPT/kochatgpt_1_SFT.jsonl\",\n",
    "    \"korquad-chat-v1/korquad-chat.json\",\n",
    "    \"OIG-small-chip2-ko/oig-smallchip2-dedu.jsonl\",\n",
    "    \"ShareGPT_DeepL/ko_alpaca_style_dataset.json\",\n",
    "    \"kullm_v2/kullm-v2.jsonl\",\n",
    "    \"KoAlpaca/ko_alpaca_data.json\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoAlpaca v1.0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49620"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[-1]\n",
    "\n",
    "with open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    koalpaca_v1_0 = json.load(f)\n",
    "\n",
    "len(koalpaca_v1_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoAlpaca v1.1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21155"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[0]\n",
    "\n",
    "koalpaca_v1_1 = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        koalpaca_v1_1.append(line)\n",
    "\n",
    "len(koalpaca_v1_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KochatGPT SFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[1]\n",
    "with open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    kochatgpt = json.load(f)\n",
    "\n",
    "len(kochatgpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korquad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9619"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[2]\n",
    "\n",
    "korquad = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        korquad.append(line)\n",
    "\n",
    "len(korquad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OIG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210282"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[3]\n",
    "\n",
    "oig = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        oig.append(line)\n",
    "\n",
    "len(oig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShareGPT Korean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_486168/3810352176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msharegpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharegpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = data_list[4]\n",
    "\n",
    "with open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    sharegpt = json.load(f)\n",
    "\n",
    "len(sharegpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KULLM-v2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152630"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = data_list[5]\n",
    "\n",
    "kullm = []\n",
    "with jsonlines.open(os.path.join(DATA_DIR, data_path), \"r\") as f:\n",
    "    for line in f.iter():\n",
    "        kullm.append(line)\n",
    "\n",
    "len(kullm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?', 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}\n",
      "{'prompt': '불고기용 고기 한우에요?', 'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\", 'tokens': 193}\n",
      "{'source': 'korquad-chat', 'text': '<sys>1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\\n<usr> 바그너가 1839년에 파우스트를 소재로 한 교향곡 작곡을 시작했다는데, 왜 이 소재에 마음이 끌렸을까요?\\n<bot> 바그너는 파우스트의 메피스토펠레스를 만나는 파우스트의 심경에 공감했기 때문입니다. 또한 바그너는 파리에서 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았고, 이것이 이 작품에 조금 영향을 끼쳤다고 합니다.\\n<usr> 작곡을 시작한 이후 작업은 어떻게 진행됐나요?\\n<bot> 작곡을 시작한 1839년부터 40년에 걸쳐 파리에서 작업을 시작했지만, 1악장을 쓴 뒤 중단하게 됐습니다. 그리고 이 간 동안 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 바쁜 시간을 보냈는데 이런 바쁜 생활이 이 곡을 잊게 한 것이 아닐까 추측되고 있습니다.\\n<usr> 그렇다면 이 작품이 연주되지 않은 이유는 무엇인가요?\\n<bot> 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였지만, 실제로는 이루어지지 않았습니다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고, 재연도 이루어졌지만, 이후에는 그대로 방치되고 말았습니다.'}\n",
      "{'user': \"I've heard that it's a good idea to have a will. What is a will?\\n\\n\", 'chip2': \"A will is a legal document that specifies how your property should be distributed after you die. It can also specify who should care for any children or other dependents you may have. It's important to make sure that your will is valid and up-to-date, since the laws governing wills vary from state to state.\", 'index': 0, 'user_translated': '유언장이 있는 것이 좋다는 말을 들었습니다. 유언장이란 무엇입니까?\\n\\n', 'chip2_translated': '유언장은 귀하가 사망한 후 귀하의 재산이 어떻게 분배되어야 하는지를 지정하는 법적 문서입니다. 또한 귀하가 가질 수 있는 자녀나 기타 부양가족을 누가 돌봐야 하는지 명시할 수 있습니다. 유언장에 적용되는 법률이 주마다 다르기 때문에 귀하의 유언장이 유효하고 최신인지 확인하는 것이 중요합니다.'}\n",
      "{'id': 'vicuna_{idx}', 'instruction': '이것이 난수를 생성하는 가장 좋은 방법일까요?const code = Math.floor(100000 + Math.random() \\\\* 900000);', 'input': '', 'output': \"제공한 코드입니다:```const code = Math.floor(100000 + Math.random() * 900000);```는 100000에서 999999 사이의 난수를 생성하므로 사용 사례에 적합할 수 있습니다. 그러나 이 방법은 난수를 생성하는 암호학적으로 안전한 방법으로 간주되지 않습니다.대신 암호학적으로 안전한 난수 생성기(CSPRNG)를 사용하는 것이 좋습니다. Node.js의 `crypto` 모듈은 암호학적으로 강력한 의사 난수 데이터를 생성하는 `randomBytes()` 함수를 제공합니다.다음은 `crypto.randomBytes()`를 사용하여 임의의 6자리 코드를 생성하는 방법의 예입니다:```const crypto = require('crypto');const code = crypto.randomBytes(3).toString('hex');console.log(code);```이렇게 하면 6개의 16진수 문자(0-9 및 a-f)로 구성된 임의의 문자열이 생성되며, 이 문자열을 코드로 사용할 수 있습니다.생성된 난수는 비밀로 유지되어야 하며 일반 텍스트로 저장해서는 안 된다는 점을 명심해야 합니다.\"}\n"
     ]
    }
   ],
   "source": [
    "# print(koalpaca_v1_0[0])\n",
    "print(koalpaca_v1_1[0])\n",
    "print(kochatgpt[0])\n",
    "print(korquad[0])\n",
    "print(oig[0])\n",
    "# print(sharegpt[0])\n",
    "print(kullm[-11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unifying except for Korquad dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21155/21155 [00:03<00:00, 6393.07it/s] \n",
      "100%|██████████| 12000/12000 [00:01<00:00, 6788.38it/s] \n",
      "100%|██████████| 210282/210282 [00:15<00:00, 13576.55it/s]\n",
      "100%|██████████| 152630/152630 [00:29<00:00, 5132.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "381020"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def to_dialog(data, inst_key, out_key):\n",
    "    dialog = []\n",
    "    for sample in tqdm(data, total=len(data)):\n",
    "        output = re.sub(r\".(\\d).\", r\".\\n\\1.\", sample[out_key])\n",
    "        \n",
    "        dialog.append(\n",
    "            [{\"from\": \"human\", \"value\": sample[inst_key]},\n",
    "            {\"from\": \"bot\", \"value\": output}]\n",
    "        )\n",
    "    \n",
    "    return dialog\n",
    "        \n",
    "\n",
    "print(\"Unifying except for Korquad dataset\")\n",
    "unified_data = []\n",
    "\n",
    "# koalpacav1.1\n",
    "unified_data.extend(to_dialog(koalpaca_v1_1, \"instruction\", \"output\"))\n",
    "# kochatgpt\n",
    "unified_data.extend(to_dialog(kochatgpt, \"prompt\", \"completion\"))\n",
    "# OIG\n",
    "unified_data.extend(to_dialog(oig, \"user_translated\", \"chip2_translated\"))\n",
    "# ShareGPT\n",
    "# unified_data.extend(to_dialog(sharegpt, \"instruction\", \"output\"))\n",
    "    \n",
    "# KULLM-v2\n",
    "processed_kullm = []\n",
    "for sample in tqdm(kullm, total=len(kullm)):\n",
    "    if sample[\"input\"]:\n",
    "        inst = sample[\"instruction\"].strip()+\"\\n\\n\"+sample[\"input\"].strip()\n",
    "    else:\n",
    "        inst = sample[\"instruction\"].strip()\n",
    "        \n",
    "    if len(inst) <= 15:\n",
    "        continue\n",
    "    \n",
    "    output = re.sub(r\".(\\d).\", r\".\\n\\1.\", sample[\"output\"])\n",
    "        \n",
    "    processed_kullm.append(\n",
    "        [{\"from\": \"human\", \"value\": inst},\n",
    "         {\"from\": \"bot\", \"value\": output}]\n",
    "    )\n",
    "print(len(processed_kullm))\n",
    "unified_data.extend(processed_kullm)\n",
    "\n",
    "# unified_data.extend(koalpaca_v1_0)\n",
    "    \n",
    "len(unified_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Koquard dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390248"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in korquad:#, total=len(korquad)):\n",
    "    try:\n",
    "        turns = sample[\"text\"].split(\"\\n\")\n",
    "        context = turns[0].split(\">\")[1].strip()\n",
    "        dialog = []\n",
    "        for turn in turns[1:]:\n",
    "            splited = turn.split(\">\")\n",
    "            role = splited[0][1:]\n",
    "            text = splited[1].strip()\n",
    "            dialog.append({\"role\": role, \"text\": text})\n",
    "            \n",
    "        assert dialog[0][\"role\"]==\"usr\" and len(dialog)%2==0\n",
    "        \n",
    "        inst_res = []\n",
    "        for i in range(0, len(dialog), 2):\n",
    "            user = dialog[i][\"text\"]\n",
    "            if i==0:\n",
    "                user = context.strip()+\"\\n\\n\"+user.strip()\n",
    "            bot = dialog[i+1][\"text\"]\n",
    "        \n",
    "            inst_res.extend(\n",
    "                [{\"from\": \"human\", \"value\": user},\n",
    "                 {\"from\": \"bot\", \"value\": bot}]\n",
    "            )\n",
    "        \n",
    "        unified_data.append(inst_res)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "len(unified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    "    AutoConfig,\n",
    "    CONFIG_MAPPING\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from model_utils.conversation import get_conv_template, SeparatorStyle\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conv_template(\"elementgpt_for_general\")\n",
    "    roles = {\"human\": conv.roles[0], \"bot\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "        \n",
    "    # Tokenize conversations\n",
    "    input_ids = []\n",
    "    targets = []\n",
    "    for i, conversation in enumerate(tqdm(conversations, desc=\"Tokenizing inputs\")):\n",
    "        input_ids_sample = tokenizer(\n",
    "            conversation,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        input_ids.append(input_ids_sample)\n",
    "        targets.append(input_ids_sample.clone())\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    print(\"Masking targets...\")\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in tqdm(zip(conversations, targets), total=len(conversations)):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum()) + 1\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            round_len = len(tokenizer(rou).input_ids)\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        if False:\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_INDEX\n",
    "                print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\",\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "                print(conversation)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing inputs: 100%|██████████| 118/118 [00:00<00:00, 460.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masking targets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2669 > 1024). Running this sequence through the model will result in indexing errors\n",
      " 37%|███▋      | 44/118 [00:00<00:00, 438.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tokenization mismatch: 1 vs. 507.  (ignored)\n",
      "호기심 많은 유저와 인공지능 어시스턴트 간의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 정중한 답변을 합니다. USER:ASSISTANT: instruction\n",
      "다음은 이 SRT에 대한 분석입니다.\n",
      "1. 부터:.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "6. -.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "9.* 이.\n",
      "1.은 발표자가 청중에게 인사하고 참석에 대한 감사를 표하는 것으로 시작됩니다.* 다음으로 발표자는 자신이 활동가는 아니지만 청중이 영감을 얻을 수 있기를 바라며 자신의 이야기를 나누고 싶다고 말합니다.* 이 부분은 연설의 시작점이며 청중이 발표자에 대한 관심을 불러일으키기 위한 것입니다.\n",
      "1. 약속:.\n",
      "0..\n",
      "0..\n",
      "1.7 -.\n",
      "0..\n",
      "1..\n",
      "1..\n",
      "9.* 이 섹션에서 발표자는 어릴 적 어머니의 노력에 영감을 받아 어릴 때부터 배우를 꿈꾸게 된 자신의 성장 경험을 공유하기 시작합니다.* 이 섹션은 전체 연설에서 이어지며 청중에게 발표자의 배경과 동기를 소개합니다.\n",
      "1. 턴:.\n",
      "0..\n",
      "1..\n",
      "1.4 -.\n",
      "0..\n",
      "1..\n",
      "3..\n",
      "6.* 이 섹션은 연설의 전환점입니다. 발표자는 어렸을 때부터 연기를 해왔다고 밝히며 자신이 출연했던 어린이 프로그램을 공유합니다.* 자신의 연기 경력에 대해 이야기하던 중 화자가 갑자기 감정이 격해진 듯 잠시 멈춘 후 계속합니다.* 이 부분은 발표자의 연기 경력에 감정적인 요소를 도입하여 전체 연설에 깊이와 의미를 더했습니다.\n",
      "1. 합산:.\n",
      "0..\n",
      "1..\n",
      "3..\n",
      "6. -.\n",
      "0..\n",
      "2..\n",
      "5..\n",
      "0.* 이 섹션에서 발표자는 계속해서 자신의 성장 경험을 공유하고 연기에 대한 사랑과 경력에 대한 추구를 강조합니다.* 발표자는 청중의 경청과 지지에 감사를 표하고 자신의 이야기가 청중에게 영감을 주었으면 좋겠다는 바람을 표현하며 마무리합니다.* 이 섹션은 연설을 마무리하며 연설의 내용을 요약하고 다음과 같은 연설자의 열정과 목표를 강조하는 것을 목표로 합니다.<|endoftext|>\n",
      "WARNING: tokenization mismatch: 1 vs. 934.  (ignored)\n",
      "호기심 많은 유저와 인공지능 어시스턴트 간의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 정중한 답변을 합니다. USER:ASSISTANT: instruction\n",
      "다음은 고객님께서 제공하신 SRT를 기반으로 한.\n",
      "1. 하이라이트 클립입니다.\n",
      "1..\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "8.6 -->.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "4.6안녕하십니.\n",
      "2..\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "1.6 -->.\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "4.6만나서 반가워요, 좋은 밤 보내세요.\n",
      "3..\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "3.3 -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7..\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7.3 여기 오게 돼서 영광이에.\n",
      "4..\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7.3 -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "9.6여기 와서 정말 행복해요.\n",
      "5..\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "1.3 -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "0.6 내 이야기를 조금 들려줄게.\n",
      "6..\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "4.0 -->.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "3.3여러분 한 사람 한 사.\n",
      "7..\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "2.0 -->.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "1.3내가 할 수 있다고 느끼기 때문.\n",
      "8..\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "9.6 -->.\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "5.3우리 엄마가.\n",
      "1.살 때 날 낳았.\n",
      "9..\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "7.3 -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7..\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7.6 그리고 저는 텍사스 그랜드 페리 출신입니다.\n",
      "1...\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7.3 -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "1..\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7.3 -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "1.6 그래서 저에게 그녀는 강한 여성의 정의입니다.\n",
      "1...\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "6.6 -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "4.9고마워.\n",
      "1...\n",
      "0..\n",
      "0..\n",
      "4..\n",
      "1.6 -->.\n",
      "0..\n",
      "0..\n",
      "4..\n",
      "7.9그리고 바니에서.\n",
      "1. 할 때부터 알았어요.\n",
      "1...\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "9.6 -->.\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "8.9내가.\n",
      "1.살 .\n",
      "1...\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "0.6 -->.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "0.6 -->.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "5.9 요점은 내가 언제였냐는 것이다.\n",
      "1...\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "7.3 -->.\n",
      "0..\n",
      "0..\n",
      "0..\n",
      "6..\n",
      "7.저는 배우가 되고 싶었어요...이.\n",
      "1. 하이라이트에서 발표자는 다음과 같은 점을 강조했습니다:* 참석자들에 대한 환영과 감사를 표현했습니다.* 자신의 배경과 성장 배경에 대해 간략하게 설명했습니다.* 어머니가 자신에게 끼친 영향과 어머니에 대한 사랑을 강조했습니다.* 어렸을 때부터 배우가 되고 싶었다고 언급.이 발췌문에서 우리는 환영과 감사의 인사, 어머니에 대한 감사, 자신의 양육과 꿈에 대한 언급 등 연속성의 요소를 볼 수 있습니다. 동시에 이러한 요소는 맥락에 맞게 논리적으로 구성되어 있습니다.<|endoftext|>\n",
      "WARNING: tokenization mismatch: 1 vs. 884.  (ignored)\n",
      "호기심 많은 유저와 인공지능 어시스턴트 간의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 정중한 답변을 합니다. USER:ASSISTANT: instruction\n",
      "[...].\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "0.6 -->.\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "5.6마틴 루터 킹 주니어 박.\n",
      "2.0.\n",
      "0..\n",
      "1..\n",
      "1.3 -->.\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "0..\n",
      "1..\n",
      "8..\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "3.6 -->.\n",
      "0..\n",
      "0..\n",
      "1..\n",
      "6.9시민권 운동에 대해 반성하는 연설...\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "9..\n",
      "0. -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "5.3그리고 나는 깨달았.\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "0..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "7.9학생들에게 발언을 요청하려.\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "3..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "9.3나는 진실을 말해야만 했.\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "5..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "5.3그리고 솔직하게--.\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "6..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "2..\n",
      "3.6내가 그렇게 하지 못한 시간들에 대.\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "9..\n",
      "0. -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "7.6그래서 나는 자라면.\n",
      "3..\n",
      "0..\n",
      "1..\n",
      "1..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "7.6오를레앙의 가톨릭 가정에서 어린 시절을 보냈습니다.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "4..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "1.3사순절 기간 동안...\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "5..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "4.9저는 항상 가장 의미 있는 것.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "7..\n",
      "0. -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "3.6사람이 할 수 있는 .\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "8..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "3..\n",
      "8.6무언가를 포기하는 것이었.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "0..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "4..\n",
      "3.3그래서 말하기를 포기하기로 했어.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "4..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "4..\n",
      "3.6내가 할 수 있는 가장 가치 있는 일.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "5..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "4..\n",
      "4.3다른 사람의 말을 경청하는 거였어.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "7..\n",
      "6. -->.\n",
      "0..\n",
      "0..\n",
      "4..\n",
      "5.3내 반 친구들과 선생님--.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "9..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "5.6내 침묵에 매우 불편해했다.\n",
      "4..\n",
      "0..\n",
      "1..\n",
      "2..\n",
      "3. -->.\n",
      "0..\n",
      "0..\n",
      "5..\n",
      "4.0그들은 내가 참여하기를 원했.\n",
      "5..\n",
      "0..\n",
      "1..\n",
      "5.<|endoftext|>\n",
      "WARNING: tokenization mismatch: 1 vs. 51.  (ignored)\n",
      "호기심 많은 유저와 인공지능 어시스턴트 간의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 정중한 답변을 합니다. USER:ASSISTANT: instruction\n",
      "<|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:00<00:00, 500.02it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"EleutherAI/polyglot-ko-1.3b\",\n",
    "    cache_dir=None,\n",
    "    model_max_length=1024,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "processed = preprocess(unified_data[275282:275400], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'호기심 많은 유저와 인공지능 어시스턴트 간의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 정중한 답변을 합니다. USER: 언제 그녀가 관계를 재건하고 싶어한다고 생각해야 하나요?포인트 ASSISTANT: 다음은 그녀가 관계를 재건할 의향이 있음을 나타내는 몇 가지 징후입니다.\\n1. 귀하와 대화를 시작하거나 귀하가 연락을 취했을 때 긍정적으로 반응합니다.\\n2. 그녀가 다시 농담이나 친근한 상호작용을 하기 시작합니다.\\n3. 그녀가 당신과 함께 계획을 세우거나 함께 어울리자고 초대합니다.\\n4. 그녀는 귀하의 행동에 대한 긍정적인 변화를 인정하고 이에 대해 감사를 표합니다.\\n5. 과거에 상처를 준 행동에 대해 사과합니다.그러나 관계를 회복하려면 양쪽 모두의 시간과 노력이 필요하다는 점을 명심하세요. 공개적이고 정직하게 소통하고, 서로의 말에 귀를 기울이며, 발생하는 모든 문제를 기꺼이 해결하려는 자세가 중요합니다.<|endoftext|>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(processed[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/unified_instruction.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unified_data, f, ensure_ascii=False) # ensure_ascii로 한글이 깨지지 않게 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390248"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from model_utils.conversation import get_conv_template\n",
    "\n",
    "conv = get_conv_template(\"elementgpt_for_general\")\n",
    "roles = {\"human\": conv.roles[0], \"bot\": conv.roles[1]}\n",
    "\n",
    "conversations = []\n",
    "for i, source in enumerate(unified_data[-10:]):\n",
    "    if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "        # Skip the first one if it is not from human\n",
    "        source = source[1:]\n",
    "\n",
    "    conv.messages = []\n",
    "    for j, sentence in enumerate(source):\n",
    "        role = roles[sentence[\"from\"]]\n",
    "        assert role == conv.roles[j % 2], f\"{i}\"\n",
    "        conv.append_message(role, sentence[\"value\"])\n",
    "    conversations.append(conv.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'호기심 많은 유저와 인공지능 어시스턴트 간의 채팅. 어시스턴트는 사용자의 질문에 도움이 되고 상세하며 정중한 답변을 합니다. USER: 울산경찰청 지능범죄수사대는 2018년 3월 16일 자유한국당 소속 울산시장인 김기현의 측근 비리를 포착해 울산시청을 압수수색하고, 김기현 동생에 대한 체포영장을 발부받아 수사에 착수했다. 그에 대해 장제원 자유한국당 수석대변인은 2018년 3월 22일 논평을 내고 경찰을 겨냥해 \"정권의 사냥개가 광견병까지 걸려 정권의 이익을 위해서라면 닥치는 대로 물어뜯기 시작했다. 미친개는 몽둥이가 약이다\" 등의 원색적인 비난을 쏟아냈다. 이에 대해 여론이 싸늘해 지자 장제원은 2018년 3월 28일 자신의 페이스북에서 논란이 된 논평에 대해 \\'경찰 전체가 아닌 울산경찰청장을 비롯한 일부 정치 경찰을 명시한 것\\'이라고 해명하며 \\'권력을 추종하는 정치경찰들을 반드시 추방해야 한다\\'고 재차 주장했다.\\n\\n최근 언론에서 울산시장 김기현 측근 비리와 관련된 뉴스가 자주 보이는데, 그에 대한 대응책이 어떻게 될까요? ASSISTANT: 경찰이 울산시청을 압수수색하고, 김기현 시장의 측근에 대한 수사를 진행하고 있다고 합니다.<|endoftext|>USER: 그렇군요. 그런데 자유한국당 수석대변인이 경찰을 겨냥한 언급이 있다고 들었습니다. 어떻게 생각하십니까? ASSISTANT: 그에 대해 장제원 수석대변인은 경찰 전체가 아닌 몇몇 정치경찰의 문제점을 지적한 것이라고 해명하였습니다.<|endoftext|>USER: 장제원 씨는 정치경찰의 추방을 주장하고 있다는데, 이에 대해 어떻게 생각하십니까? ASSISTANT: 저는 권력을 추종하는 정치경찰이 있다면 이에 대한 대응책을 마련하고, 법을 지키며 공정한 수사를 해나가는 것이 바람직하다고 생각합니다.<|endoftext|>'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[2][]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
